Part One (part d)

The Flesch Kincaid score may not always be a valid, robust or reliable estimator for text 4 difficulty.

One such condition may include  cases where text contains non-standard vocabulary for example scientific journal articles or legalisation.
These texts usually contain specialist technical jargon which may not be easily understood by a layman. They may also contain simpler syllable patterns or shorter sentences which influences the overall score. 

Another condition is when text may be difficult to read and understand due to comprehension difficulty. The score does not account for comprehension difficulty caused by factors such as metaphorical language, cultural references and abstract concepts. 
For example poetry and literature such as Shakespeare often relies on implied meanings and metaphorical language, which is not captured by sentence structure or syllable count. 
The readability formula does not measure this so therefore such texts may score low despite being quite difficult to read and understand.


-----
Part Two (part f)

The custom tokenizer converts all text into lowercase for consistency purposes to avoid differentiating words which are capitalised from those which are not e.g. 'The' and 'the'. A word_tokenizer is then applied to divide text into lists of substrings. This allows for words and punctuation to be identified within the string. 
Once the string is divided into lists of substrings, the custom tokenizer uses regex to remove any punctuation and then uses wordnet from NLTK to remove any stopwords. Lemmatisation is then used to reduce a word into its root meaning for example the word better is reduced to good.

A linear SVM model and ridge classifier both have an accuracy of 0.83 suggesting that both models can correctly predict outcomes. SVM has a greater macro average precision which suggests when predicting positive outcomes (true positives), there is a higher probability that the prediction is correct. 
Ridge Classification has a marginally better recall which suggests that if there is a positive class, there is a marginally greater chance the model is correctly classify the positive class. It is worth noting that Ridge Classification shows better balanced performance across classes as it has greater macro-average F1 score at 0.64.

Ridge Classification shows relatively strong precision and recall when classifying Conservative and Labour speeches. As both precision and recall is high when classifying Labour and Conservative speeches, this suggests there are few false positives (i.e. if a model makes a prediction, there is a high probability the prediction is correct) and there are few false negatives (i.e. there is a high chance the model will detect it).
Additionally Ridge Classification shows relatively strong precision when classifying Liberal Democrat and Scottish National Party speeches. However it is worth noting that recall for Ridge Classification is comparable to SVM and Random Forest classification. As precision is high with lower recall, this suggests that the model may miss positive outcomes although when an outcome is classed as positive, it is highly likely it is positive (i.e. fthere are few false positives but a higher proportion of false negatives).

Ridge Classification performs the best, closely followed with SVM - especially as the macro-average F1 score is the highest amongst all models.